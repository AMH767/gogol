from flask import Flask, render_template, request, jsonify, send_file
import pandas as pd
import io
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium_stealth import stealth
from time import sleep
import os
from datetime import datetime
import threading
import uuid
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
from bs4 import BeautifulSoup
import re
import psycopg2
from psycopg2.extras import RealDictCursor
from neighborhoods import get_subregions

app = Flask(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–¥–∞—á
tasks = {}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö PostgreSQL
DATABASE_URL = os.environ.get('DATABASE_URL')

def get_db_connection():
    if not DATABASE_URL:
        return None
    # Supabase –∏ Render –∏–Ω–æ–≥–¥–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—Ç –ø–æ IPv6. 
    # –î–æ–±–∞–≤–ª—è–µ–º sslmode=require –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    url = DATABASE_URL
    if 'sslmode' not in url:
        if '?' in url:
            url += '&sslmode=require'
        else:
            url += '?sslmode=require'
    return psycopg2.connect(url)

def init_db():
    try:
        conn = get_db_connection()
        if not conn:
            print("‚ö†Ô∏è DATABASE_URL –Ω–µ –∑–∞–¥–∞–Ω. –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
            return
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS results (
                id SERIAL PRIMARY KEY,
                task_id TEXT,
                name TEXT,
                address TEXT,
                phone TEXT,
                rating TEXT,
                website TEXT,
                url TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        cursor.close()
        conn.close()
        print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ë–î –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ, –Ω–æ –Ω–µ –¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é —É–ø–∞—Å—Ç—å, –µ—Å–ª–∏ –Ω–µ—Ç —Å–µ—Ç–∏
with app.app_context():
    init_db()

def save_to_db(task_id, data):
    conn = get_db_connection()
    if not conn: return
    cursor = conn.cursor()
    cursor.execute('''
        INSERT INTO results (task_id, name, address, phone, rating, website, url)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
    ''', (task_id, data['name'], data['address'], data['phone'], data['rating'], data['website'], data['url']))
    conn.commit()
    cursor.close()
    conn.close()

def log_message(task_id, msg):
    timestamp = datetime.now().strftime("%H:%M:%S")
    clean_msg = msg.strip()
    full_msg = f"[{timestamp}] {clean_msg}"
    print(full_msg)
    if task_id in tasks:
        tasks[task_id]['logs'].append(clean_msg)
        if len(tasks[task_id]['logs']) > 500:
            tasks[task_id]['logs'].pop(0)

class GoogleMapsParser:
    def __init__(self, task_id, query, many, lang='ru', region='RU', deep_search=True):
        self.task_id = task_id
        self.query = query
        self.many = many
        self.lang = lang
        self.region = region
        self.deep_search = deep_search
        self.max_workers = 5
        self.results_lock = threading.Lock()

    def create_driver(self):
        chrome_options = webdriver.ChromeOptions()
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1200,800")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--disable-images")
        
        if os.path.exists("/usr/bin/chromium"):
            chrome_options.binary_location = "/usr/bin/chromium"
        elif os.path.exists("/usr/bin/chromium-browser"):
            chrome_options.binary_location = "/usr/bin/chromium-browser"
        
        ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
        chrome_options.add_argument(f"user-agent={ua}")
        
        try:
            driver = webdriver.Chrome(options=chrome_options)
            driver.set_page_load_timeout(30)
            stealth(driver,
                languages=[f"{self.lang}-{self.region}", self.lang, "en-US", "en"],
                vendor="Google Inc.",
                platform="Win32",
                webgl_vendor="Intel Inc.",
                renderer="Intel Iris OpenGL Engine",
                fix_hairline=True,
            )
            return driver
        except Exception as e:
            log_message(self.task_id, f"‚ùå –û—à–∏–±–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–∞: {e}")
            return None

    def get_links_for_query(self, search_query, limit):
        driver = self.create_driver()
        if not driver: return []
        
        links = set()
        try:
            url = f"https://www.google.com/maps/search/{search_query.replace(' ', '+')}?hl={self.lang}&gl={self.region}"
            driver.get(url)
            
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, '//a[contains(@href, "/maps/place/")]'))
                )
            except:
                driver.quit()
                return []

            scrollable_div = None
            selectors = ['//div[@role="feed"]', '//div[contains(@aria-label, "Results for")]']
            for s in selectors:
                try:
                    scrollable_div = driver.find_element(By.XPATH, s)
                    if scrollable_div: break
                except: continue
            
            if not scrollable_div: scrollable_div = driver.find_element(By.TAG_NAME, "body")

            last_len = 0
            no_change = 0
            while len(links) < limit and no_change < 5:
                driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)
                sleep(2)
                found = driver.find_elements(By.XPATH, '//a[contains(@href, "/maps/place/")]')
                for f in found:
                    href = f.get_attribute('href')
                    if href: links.add(href)
                
                if len(links) == last_len: no_change += 1
                else: no_change = 0; last_len = len(links)
                
        except Exception as e:
            log_message(self.task_id, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ '{search_query}': {e}")
        finally:
            driver.quit()
        return list(links)

    def parse_details(self, url):
        driver = self.create_driver()
        if not driver: return None
        
        try:
            driver.get(url)
            sleep(2)
            soup = BeautifulSoup(driver.page_source, 'lxml')
            data = {'name': 'N/A', 'address': 'N/A', 'phone': 'N/A', 'rating': 'N/A', 'website': 'N/A', 'url': url}
            
            title_el = soup.find('h1')
            if title_el: data['name'] = title_el.get_text(strip=True)
            
            rating_el = soup.select_one('span[aria-label*="star"], span[aria-label*="–∑–≤–µ–∑–¥"], span[aria-label*="–æ—Ü–µ–Ω"]')
            if rating_el:
                rt = rating_el.get('aria-label', '')
                match = re.search(r'(\d[.,]\d)', rt)
                data['rating'] = match.group(1).replace(',', '.') if match else 'N/A'
            
            if data['rating'] == 'N/A':
                rating_val = soup.select_one('.MW4etd, .ce967p')
                if rating_val:
                    data['rating'] = rating_val.get_text(strip=True).replace(',', '.')
            
            items = soup.find_all(['button', 'a'], attrs={'data-item-id': True})
            for item in items:
                item_id = item.get('data-item-id', '')
                text = item.get_text(strip=True)
                clean_text = re.sub(r'^[^\w\s\(\)\+]+', '', text).strip()
                
                if 'address' in item_id:
                    data['address'] = clean_text
                elif 'phone' in item_id:
                    phone_match = re.search(r'tel:(\+\d+)', item_id)
                    data['phone'] = phone_match.group(1) if phone_match else clean_text
                elif 'authority' in item_id:
                    data['website'] = item.get('href', clean_text)
            
            if data['website'] == 'N/A':
                site_link = soup.select_one('a[aria-label*="website"], a[aria-label*="–°–∞–π—Ç"]')
                if site_link: data['website'] = site_link.get('href', 'N/A')
                
            return data
        except Exception:
            return None
        finally:
            driver.quit()

    def run(self):
        try:
            all_links = set()
            log_message(self.task_id, f"üì° –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {self.query}")
            main_links = self.get_links_for_query(self.query, self.many)
            all_links.update(main_links)
            log_message(self.task_id, f"üîó –ù–∞–π–¥–µ–Ω–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ–∏—Å–∫–µ: {len(main_links)}")

            if self.deep_search and len(all_links) < self.many:
                subregions = get_subregions(self.query)
                if subregions:
                    log_message(self.task_id, f"üîç –í–∫–ª—é—á–µ–Ω –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫. –ù–∞–π–¥–µ–Ω–æ —Ä–∞–π–æ–Ω–æ–≤ –¥–ª—è '{self.query}': {len(subregions)}")
                    for sub in subregions:
                        if len(all_links) >= self.many: break
                        sub_query = f"{self.query} {sub}"
                        log_message(self.task_id, f"üìç –ü–æ–∏—Å–∫ –≤ —Ä–∞–π–æ–Ω–µ: {sub}")
                        sub_links = self.get_links_for_query(sub_query, self.many - len(all_links))
                        all_links.update(sub_links)
                        log_message(self.task_id, f"üîó –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ —Å–æ–±—Ä–∞–Ω–æ: {len(all_links)}")
                else:
                    log_message(self.task_id, "‚ÑπÔ∏è –†–∞–π–æ–Ω—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –≥–æ—Ä–æ–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –±–∞–∑–µ, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –æ—Å–Ω–æ–≤–Ω—ã–º —Å–ø–∏—Å–∫–æ–º.")

            links_to_parse = list(all_links)[:self.many]
            if not links_to_parse:
                log_message(self.task_id, "‚ùå –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                tasks[self.task_id]['status'] = 'completed'
                return

            log_message(self.task_id, f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ {len(links_to_parse)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π...")
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(self.parse_details, url) for url in links_to_parse]
                for future in concurrent.futures.as_completed(futures):
                    res = future.result()
                    if res:
                        with self.results_lock:
                            tasks[self.task_id]['results'].append({"id": len(tasks[self.task_id]['results']) + 1, **res})
                            save_to_db(self.task_id, res)
                            log_message(self.task_id, f"‚úÖ –°–ø–∞—Ä—Å–µ–Ω–æ:\n{res['name']}\nüìç {res['address']}\nüìû {res['phone']}\nüåê {res['website']}\n---")

            tasks[self.task_id]['end_time'] = datetime.now()
            duration = tasks[self.task_id]['end_time'] - tasks[self.task_id]['start_time']
            minutes, seconds = divmod(duration.total_seconds(), 60)
            time_str = f"{int(minutes)}–º {int(seconds)}—Å" if minutes > 0 else f"{int(seconds)}—Å"
            
            log_message(self.task_id, f"‚úÖ –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω! –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {time_str}")
            tasks[self.task_id]['status'] = 'completed'
        except Exception as e:
            tasks[self.task_id]['end_time'] = datetime.now()
            log_message(self.task_id, f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            tasks[self.task_id]['status'] = 'error'

@app.route('/')
def index():
    return render_template('index_google.html')

@app.route('/parse', methods=['POST'])
def parse():
    data = request.json
    task_id = str(uuid.uuid4())
    tasks[task_id] = {
        'status': 'running',
        'logs': [],
        'results': [],
        'start_time': datetime.now(),
        'query': data.get('query', ''),
        'many': int(data.get('many', 10))
    }
    
    parser = GoogleMapsParser(
        task_id, 
        data.get('query'), 
        int(data.get('many', 10)),
        lang=data.get('lang', 'ru'),
        region=data.get('region', 'RU'),
        deep_search=data.get('deep_search', True)
    )
    threading.Thread(target=parser.run).start()
    return jsonify({'task_id': task_id})

@app.route('/status/<task_id>')
def status(task_id):
    if task_id not in tasks:
        return jsonify({'error': 'Task not found'}), 404
    return jsonify(tasks[task_id])

@app.route('/history')
def history():
    conn = get_db_connection()
    if not conn:
        return render_template('history.html', results=[])
    cursor = conn.cursor(cursor_factory=RealDictCursor)
    cursor.execute('SELECT * FROM results ORDER BY timestamp DESC LIMIT 1000')
    results = cursor.fetchall()
    cursor.close()
    conn.close()
    return render_template('history.html', results=results)

@app.route('/export/<task_id>')
def export(task_id):
    # Fetch results for this task from DB
    conn = get_db_connection()
    if not conn:
        return "Database not connected", 500
    cursor = conn.cursor(cursor_factory=RealDictCursor)
    cursor.execute('SELECT name, address, phone, rating, website, url FROM results WHERE task_id = %s', (task_id,))
    results = cursor.fetchall()
    cursor.close()
    conn.close()
    
    if not results:
        return "No results found", 404
        
    df = pd.DataFrame(results)
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False)
    output.seek(0)
    
    return send_file(
        output,
        mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        as_attachment=True,
        download_name=f'results_{task_id}.xlsx'
    )

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=10000)
